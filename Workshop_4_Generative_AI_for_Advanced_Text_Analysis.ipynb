{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JJingLu/CBS5055-Generative-Artificial-Intelligence-for-Innovative-Communications/blob/main/Workshop_4_Generative_AI_for_Advanced_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20v_766-6ZdM"
      },
      "source": [
        "## Workshop 4: Generative AI for Advanced Text Analysis CBS5055\n",
        "\n",
        "**Instructor: Jessie Lu**  \n",
        "\n",
        "Welcome to Workshop 4!  \n",
        "\n",
        "In todayâ€™s session, you will learn the efficient fine-tuning technique called LoRA, enabling the model to better adapt to your data. You will get hands-on experience applying the fine-tuned model to real data for sentiment analysis, and you will learn how to interpret the analysis results.\n",
        "\n",
        "\n",
        "\n",
        "The specific dataset we will explore today is **go_emotions** , originally created by Google Research and published on Hugging Face. It contains approximately 58,000 Reddit comments, each annotated (by human raters) with one or more of 28 fine-grained emotion categories. This dataset is widely used in emotion detection, affective computing, and social media analysis research.  \n",
        "\n",
        "You can view and explore the dataset directly here:  \n",
        "*https://huggingface.co/datasets/google-research-datasets/go_emotions*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MOIzp2y6T1p"
      },
      "outputs": [],
      "source": [
        "#@title 1. Install Required Libraries\n",
        "!pip install -q \\\n",
        "    transformers>=4.30.0 \\\n",
        "    datasets>=2.12.0 \\\n",
        "    peft>=0.4.0 \\\n",
        "    tqdm>=4.65.0 \\\n",
        "    scikit-learn>=1.2.2 \\\n",
        "    torch>=2.0.0 \\\n",
        "    matplotlib>=3.7.0 \\\n",
        "    seaborn>=0.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_GflmU26VKc"
      },
      "outputs": [],
      "source": [
        "#@title 2. Import Libraries\n",
        "from datasets import load_dataset # Import function to load datasets\n",
        "from transformers import (\n",
        "    DistilBertTokenizer, # Import tokenizer for DistilBERT\n",
        "    DistilBertForSequenceClassification, # Import DistilBERT model for sequence classification\n",
        "    get_linear_schedule_with_warmup # Import learning rate scheduler\n",
        ")\n",
        "from torch.optim import AdamW # Import AdamW optimizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType # Import PEFT utilities for LoRA\n",
        "from tqdm.auto import tqdm # Import tqdm for progress bars\n",
        "import torch # Import PyTorch library\n",
        "from torch.nn.utils import clip_grad_norm_ # Import gradient clipping utility\n",
        "from sklearn.metrics import accuracy_score # Import accuracy metric from scikit-learn\n",
        "from torch.utils.data import DataLoader # Import DataLoader for batching data\n",
        "import os # Import os module for interacting with the operating system\n",
        "from pathlib import Path # Import Path for object-oriented filesystem paths\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "import seaborn as sns # Import seaborn for enhanced data visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-t7CtY87Mk7"
      },
      "outputs": [],
      "source": [
        "#@title 3. Directory Setup\n",
        "SAVE_DIR = Path(\"saved_data\") # Define the base directory for saving data\n",
        "MODEL_DIR = SAVE_DIR / \"model\" # Define the directory for saving models\n",
        "DATASET_DIR = SAVE_DIR / \"dataset\" # Define the directory for saving datasets\n",
        "TOKENIZED_DIR = SAVE_DIR / \"tokenized_dataset\" # Define the directory for saving tokenized datasets\n",
        "LORA_DIR = MODEL_DIR / \"trained_LoRA\" # Define the directory for saving trained LoRA models\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True) # Create the base save directory if it doesn't exist\n",
        "os.makedirs(MODEL_DIR, exist_ok=True) # Create the model save directory if it doesn't exist\n",
        "os.makedirs(DATASET_DIR, exist_ok=True) # Create the dataset save directory if it doesn't exist\n",
        "os.makedirs(TOKENIZED_DIR, exist_ok=True) # Create the tokenized dataset save directory if it doesn't exist\n",
        "os.makedirs(LORA_DIR, exist_ok=True) # Create the LoRA model save directory if it doesn't exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGGxwqpF7PaI"
      },
      "outputs": [],
      "source": [
        "#@title 4. Load Dataset\n",
        "dataset_path = DATASET_DIR / \"go_emotions_simplified\" # Define the path for the simplified go_emotions dataset\n",
        "if os.path.exists(dataset_path):\n",
        "    print(\"Loading cached dataset...\") # Inform the user that a cached dataset is being loaded\n",
        "    dataset = load_dataset(\"go_emotions\", \"simplified\", cache_dir=str(dataset_path)) # Load dataset from cache\n",
        "else:\n",
        "    print(\"Downloading dataset...\") # Inform the user that the dataset is being downloaded\n",
        "    dataset = load_dataset(\"go_emotions\", \"simplified\") # Download the go_emotions dataset\n",
        "    dataset.save_to_disk(str(dataset_path)) # Save the downloaded dataset to disk\n",
        "\n",
        "# Get number of unique labels\n",
        "num_labels = len(set(label for example in dataset['train'] for label in example['labels'])) # Calculate the number of unique emotion labels\n",
        "print(f\"Number of emotion labels: {num_labels}\") # Print the total number of unique emotion labels\n",
        "\n",
        "# Define emotion mapping\n",
        "EMOTIONS = [\n",
        "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\",\n",
        "    \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\",\n",
        "    \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\",\n",
        "    \"joy\", \"love\", \"nervousness\", \"neutral\", \"optimism\", \"pride\", \"realization\",\n",
        "    \"relief\", \"remorse\", \"sadness\", \"surprise\"\n",
        "] # Define a list of 28 fine-grained emotion categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHojc5l37R75"
      },
      "outputs": [],
      "source": [
        "#@title 5. Initialize Model and Tokenizer\n",
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\") # Load the pre-trained DistilBERT tokenizer\n",
        "\n",
        "# Initialize or load the model\n",
        "model_path = LORA_DIR / \"distilbert_lora_go_emotions\" # Define the path where the LoRA model might be saved\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading saved LoRA adapter...\") # Inform user that a saved LoRA adapter is being loaded\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        str(model_path),\n",
        "        num_labels=num_labels # Initialize model with the correct number of labels\n",
        "    )\n",
        "else:\n",
        "    print(\"Initializing new model...\") # Inform user that a new model is being initialized\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\", # Load the pre-trained DistilBERT base model\n",
        "        num_labels=num_labels # Initialize model with the correct number of labels\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcqpduRQ7UVc"
      },
      "outputs": [],
      "source": [
        "#@title 6. Configure and Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS, # Define the task type as Sequence Classification\n",
        "    r=8, # Set the LoRA attention dimension (rank)\n",
        "    lora_alpha=32, # Set the scaling factor for LoRA weights\n",
        "    lora_dropout=0.1, # Set the dropout probability for LoRA layers\n",
        "    bias=\"none\", # Specify that no bias will be trained\n",
        "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"] # Specify the modules to apply LoRA to\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config) # Apply the LoRA configuration to the model\n",
        "model.print_trainable_parameters() # Print the number of trainable parameters after applying LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3g-YH6S7WZj"
      },
      "outputs": [],
      "source": [
        "#@title 7. Data Preprocessing\n",
        "def tokenize_function(example):\n",
        "    tokenized = tokenizer(\n",
        "        example['text'], # Input text to the tokenizer\n",
        "        padding='max_length', # Pad sequences to the maximum length\n",
        "        truncation=True, # Truncate sequences longer than max_length\n",
        "        max_length=64, # Set the maximum sequence length\n",
        "        return_tensors=None # Return Python lists/arrays, not PyTorch tensors yet\n",
        "    )\n",
        "    # Handle labels (take the first label for simplicity)\n",
        "    tokenized['labels'] = [labels[0] if labels else 0 for labels in example['labels']] # Assign the first label or 0 if no labels are present\n",
        "    return tokenized # Return the tokenized example with labels\n",
        "\n",
        "batch_size = 64 # Define the batch size for data loaders\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenized_path = TOKENIZED_DIR / \"tokenized_dataset\" # Define the path for saving tokenized dataset\n",
        "if os.path.exists(tokenized_path):\n",
        "    print(\"Loading cached tokenized dataset...\") # Inform user that a cached dataset is being loaded\n",
        "    tokenized_dataset = load_dataset(\"go_emotions\", \"simplified\", cache_dir=str(tokenized_path)) # Load tokenized dataset from cache\n",
        "    # Re-tokenize as the previous save might not be in the right format\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True, batch_size=batch_size*4, remove_columns=dataset[\"train\"].column_names) # Apply tokenization function to the dataset again\n",
        "else:\n",
        "    print(\"Tokenizing dataset...\") # Inform user that the dataset is being tokenized\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function, # Apply the tokenization function\n",
        "        batched=True, # Process examples in batches\n",
        "        batch_size=batch_size * 4, # Set batch size for map function\n",
        "        remove_columns=dataset[\"train\"].column_names # Remove original text and labels columns\n",
        "    )\n",
        "    tokenized_dataset.save_to_disk(str(tokenized_path)) # Save the tokenized dataset to disk\n",
        "\n",
        "# Set format for PyTorch\n",
        "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]) # Convert dataset columns to PyTorch tensors\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(tokenized_dataset[\"train\"], batch_size=batch_size, shuffle=True) # Create a DataLoader for the training set\n",
        "eval_loader = DataLoader(tokenized_dataset[\"validation\"], batch_size=batch_size) # Create a DataLoader for the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjKSzilZ7ZWr"
      },
      "outputs": [],
      "source": [
        "#@title 8. Train the Model\n",
        "# Setup\n",
        "import torch.cuda.amp as amp # Import Automatic Mixed Precision utilities\n",
        "scaler = amp.GradScaler() # Initialize a gradient scaler for mixed precision training\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # Determine if CUDA (GPU) is available, otherwise use CPU\n",
        "print(f\"Using device: {device}\") # Confirm which device is being used\n",
        "model.to(device) # Move the model to the selected device\n",
        "optimizer = AdamW(model.parameters(), lr=4e-5) # Initialize the AdamW optimizer with a learning rate\n",
        "max_grad_norm = 1.0 # Define the maximum gradient norm for clipping\n",
        "num_epochs = 3 # Set the number of training epochs\n",
        "\n",
        "num_training_steps = len(train_loader) * num_epochs # Calculate the total number of training steps\n",
        "num_warmup_steps = num_training_steps // 10 # Calculate the number of warmup steps for the scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps) # Initialize the learning rate scheduler\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    epoch_loss = 0 # Initialize loss for the current epoch\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\") # Create a progress bar for the training loader\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()} # Move batch tensors to the appropriate device\n",
        "        optimizer.zero_grad() # Reset gradients for the current iteration (moved before forward pass for mixed precision)\n",
        "\n",
        "        with amp.autocast(enabled=device.type == 'cuda'): # Enable automatic mixed precision only if CUDA is available\n",
        "            outputs = model(**batch) # Perform a forward pass\n",
        "            loss = outputs.loss # Get the loss from the model outputs\n",
        "\n",
        "        scaler.scale(loss).backward() # Scale loss and perform backpropagation\n",
        "        scaler.unscale_(optimizer) # Unscale gradients before clipping\n",
        "        clip_grad_norm_(model.parameters(), max_grad_norm) # Clip gradients to prevent exploding gradients\n",
        "        scaler.step(optimizer) # Update model parameters using the scaled gradients\n",
        "        scaler.update() # Update the scale for the next iteration\n",
        "        scheduler.step() # Update the learning rate scheduler\n",
        "\n",
        "        epoch_loss += loss.item() # Accumulate the loss for the epoch\n",
        "        progress_bar.set_postfix({\"loss\": f\"{epoch_loss/(progress_bar.n+1):.4f}\"}) # Update the progress bar with current average loss\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader) # Calculate the average loss for the epoch\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\") # Print the average loss for the epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDvAQN7c7bve"
      },
      "outputs": [],
      "source": [
        "#@title 9. Evaluate the Model\n",
        "# Before running this cell, please ensure that cells 5, 6, and 8 have been executed successfully.\n",
        "model.eval() # Set the model to evaluation mode\n",
        "all_predictions = [] # Initialize a list to store all model predictions\n",
        "all_labels = [] # Initialize a list to store all true labels\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculations for inference\n",
        "    for batch in tqdm(eval_loader, desc=\"Evaluating\"): # Iterate over the evaluation data loader with a progress bar\n",
        "        batch = {k: v.to(device) for k, v in batch.items()} # Move batch tensors to the appropriate device (CPU/GPU)\n",
        "        outputs = model(**batch) # Perform a forward pass to get model outputs\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1) # Get the predicted class by finding the maximum logit\n",
        "        all_predictions.extend(predictions.cpu().numpy()) # Store predictions, moving them to CPU and converting to NumPy array\n",
        "        all_labels.extend(batch['labels'].cpu().numpy()) # Store true labels, moving them to CPU and converting to NumPy array\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions) # Calculate the accuracy score\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\") # Print the calculated validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxlc0rXo7dxl"
      },
      "outputs": [],
      "source": [
        "#@title 10. Model Prediction & Visualization\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\").to(device) # Tokenize the input text and move to device\n",
        "    with torch.no_grad(): # Disable gradient calculations for inference\n",
        "        outputs = model(**inputs) # Get model outputs (logits)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1) # Get the predicted class by finding the maximum logit\n",
        "    return EMOTIONS[prediction.item()] # Return the emotion label corresponding to the prediction\n",
        "\n",
        "# Test with examples\n",
        "examples = [\n",
        "    \"I'm so excited about this workshop!\",\n",
        "    \"This is the worst experience ever.\",\n",
        "    \"The meeting is scheduled for 3 PM.\",\n",
        "    \"Thank you for your help!\",\n",
        "    \"I love spending time with my family.\"\n",
        "]\n",
        "\n",
        "for text in examples:\n",
        "    print(f\"Text: {text}\") # Print the input text\n",
        "    print(f\"Predicted Emotion: {predict_sentiment(text)}\\n\") # Print the predicted emotion\n",
        "\n",
        "# Visualization: Emotion Distribution in Training Data\n",
        "df = dataset[\"train\"].to_pandas() # Convert the training dataset to a pandas DataFrame\n",
        "df[\"emotion\"] = df[\"labels\".apply(lambda x: EMOTIONS[x[0]] if len(x) > 0 else \"neutral\") # Map numerical labels to emotion names, handling empty lists explicitly\n",
        "emotion_counts = df[\"emotion\"].value_counts().head(10) # Get the top 10 most common emotions and their counts\n",
        "\n",
        "plt.figure(figsize=(12, 6)) # Create a new figure with a specified size\n",
        "sns.barplot(x=emotion_counts.values, y=emotion_counts.index, palette=\"viridis\", hue=emotion_counts.index, legend=False) # Create a bar plot of emotion counts, fixing the FutureWarning\n",
        "plt.title(\"Top 10 Most Common Emotions in GoEmotions Dataset\") # Set the title of the plot\n",
        "plt.xlabel(\"Number of Comments\") # Set the label for the x-axis\n",
        "plt.ylabel(\"Emotion\") # Set the label for the y-axis\n",
        "plt.show() # Display the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W_RE7cZ7gz6"
      },
      "outputs": [],
      "source": [
        "#@title 11. Save the Model\n",
        "model.save_pretrained(str(LORA_DIR / \"distilbert_lora_go_emotions\")) # Save the trained LoRA model to the specified directory\n",
        "tokenizer.save_pretrained(str(LORA_DIR / \"distilbert_lora_go_emotions\")) # Save the tokenizer to the same directory\n",
        "print(\"Model and tokenizer saved successfully!\") # Print a success message"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMM4Pf8RTRH1qUHpO3apNRq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}